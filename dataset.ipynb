{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime, tzinfo, timedelta, timezone\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from pathlib import Path\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "import san\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def read_csvs_and_apply_timedelta(comb_dict):\n",
    "    modified_dfs = {}\n",
    "    for source, assets in comb_dict.items():\n",
    "        modified_dfs[source] = {}\n",
    "        for asset, timeframes in assets.items():\n",
    "            modified_dfs[source][asset] = {}\n",
    "            for timeframe, info in timeframes.items():\n",
    "                file_path = info['file_path']\n",
    "                timedelta_hours = info['timedelta']\n",
    "                \n",
    "                # Placeholder for loading the actual DataFrame from CSV\n",
    "                df = pd.read_csv(file_path, index_col=0, parse_dates=True) \n",
    "                df.index = df.index + pd.Timedelta(hours=timedelta_hours)\n",
    "\n",
    "                modified_dfs[source][asset][timeframe] = df\n",
    "    return modified_dfs\n",
    "\n",
    "# List to hold all the DataFrames across sources, assets, and timeframes\n",
    "def merge_dataframes(modified_dfs):\n",
    "    all_dfs = []\n",
    "    \n",
    "    for source in modified_dfs:\n",
    "        for asset in modified_dfs[source]:\n",
    "            for timeframe in modified_dfs[source][asset]:\n",
    "                # Access the DataFrame\n",
    "                df = modified_dfs[source][asset][timeframe]\n",
    "                # Append the DataFrame to our list\n",
    "                all_dfs.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames along the columns using their datetime index for alignment\n",
    "    merged_df = pd.concat(all_dfs, axis=1, ignore_index=False).sort_index()\n",
    "    \n",
    "    return merged_df\n",
    "    \n",
    "def timestamp_to_week_cycle(timestamp, milliseconds_in_week=7*24*60*60*1000):\n",
    "    # Calculate the total number of milliseconds since the beginning of the week (Monday)\n",
    "    total_milliseconds = ((timestamp.dayofweek * 24 * 60 * 60 * 1000) +\n",
    "                        (timestamp.hour * 60 * 60 * 1000) +\n",
    "                        (timestamp.minute * 60 * 1000) +\n",
    "                        (timestamp.second * 1000) +\n",
    "                        timestamp.microsecond / 1000) % milliseconds_in_week\n",
    "    \n",
    "    # Map the milliseconds to a 2Ï€ cycle\n",
    "    radians = (total_milliseconds / milliseconds_in_week) * 2 * np.pi\n",
    "    return np.sin(radians), np.cos(radians)\n",
    "\n",
    "def timestamp_to_weekday_hour(timestamp):\n",
    "    day = timestamp.weekday()\n",
    "    hour = timestamp.hour\n",
    "\n",
    "    return day, hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "san.ApiConfig.api_key = os.getenv('SAN_API_KEY') \n",
    "calls_remaining = san.api_calls_remaining()\n",
    "# should be {'month_remaining': '1200000', 'hour_remaining': '60000', 'minute_remaining': '1200'} on Max Business Plan\n",
    "print(calls_remaining)\n",
    "\n",
    "SAVE_FOLDER = './data/dataset'\n",
    "Path(SAVE_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "price_column = 'close_0060_FTM' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# #Download data from Santiment and Binance\n",
    "# !python scraper_santiment.py --coins \"BTC,FTM\" --resolutions \"1h,24h\" --start_time \"2020-07-01T00:00:00\" --end_time \"2024-03-11T00:00:00\" --endpoint_file_paths \"./data/endpoints_file_path_santiment.json\" --save_folder \"./data/test/santiment/historical\" --mode \"historical\"\n",
    "# !python scraper_binance.py --coins \"BTC,FTM\" --resolutions \"1h,1d\" --start_time \"2020-07-01T00:00:00\" --end_time \"2024-03-11T00:00:00\" --endpoint_file_paths \"./data/endpoints_file_path_binance.json\" --save_folder \"./data/test/binance/historical\" --mode \"historical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download data from Santiment and Binance\n",
    "!python scraper_santiment.py --coins \"BTC,FTM\" --resolutions \"1h,24h\" --start_time \"2020-07-01T00:00:00\" --end_time \"2024-03-11T00:00:00\" --endpoint_file_paths \"./data/endpoints_file_path_santiment.json\" --save_folder \"./data/test/santiment/historical\" --mode \"historical\"\n",
    "!python scraper_binance.py --coins \"BTC,FTM\" --resolutions \"1h,1d\" --start_time \"2020-07-01T00:00:00\" --end_time \"2024-03-11T00:00:00\" --endpoint_file_paths \"./data/endpoints_file_path_binance.json\" --save_folder \"./data/test/binance/historical\" --mode \"historical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commands for live data\n",
    "# !python scraper_binance.py --coins \"BTC,ETH,FTM\" --resolutions \"1h,1d\" --endpoint_file_paths \"./data/endpoints_file_path_binance.json\" --save_folder \"./data/test/binance/live\" --mode \"live\"\n",
    "# !python scraper_bybit.py --coins \"BTC,FTM\" --resolutions \"1h,1d\" --endpoint_file_paths \"./data/endpoints_file_path_bybit.json\" --save_folder \"./data/test/bybit/live\" --mode \"live\"\n",
    "# !python scraper_santiment.py --coins \"BTC,ETH,FTM\" --resolutions \"1h,24h\" --endpoint_file_paths \"./data/endpoints_file_path_santiment.json\" --save_folder \"./data/test/santiment/live\" --mode \"live\"\n",
    "# !python scraper_glassnode.py --coins \"BTC,ETH\" --resolutions \"1h,24h\" --endpoint_file_paths \"./data/endpoints_file_path_glassnode.json\" --save_folder \"./data/test/glassnode/live\" --mode \"live\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Uncomment comb_dict keys (platforms, coins, timeframes) used for dataset creation\n",
    "\n",
    "comb_dict = {\n",
    "    \"binance\": {\n",
    "        \"BTC\": {\n",
    "            \"1h\": {\n",
    "                \"file_path\": \"./data/test/binance/historical/scraped_binance_BTC_1h_2025-05-11_00:00:00.csv\",\n",
    "                \"timedelta\": 1\n",
    "            },\n",
    "            \"24h\": {\n",
    "                \"file_path\": \"./data/test/binance/historical/scraped_binance_BTC_1d_2025-05-11_00:00:00.csv\",\n",
    "                \"timedelta\": 24\n",
    "            }\n",
    "        },\n",
    "        # \"ETH\": {\n",
    "        #     \"1h\": {\n",
    "        #         \"file_path\": \"./data/test/binance/historical/scraped_binance_ETH_1h_2024-03-11_00:00:00.csv\",\n",
    "        #         \"timedelta\": 1\n",
    "        #     },\n",
    "        #     \"24h\": {\n",
    "        #         \"file_path\": \"./data/test/binance/historical/scraped_binance_ETH_1d_2024-03-11_00:00:00.csv\",\n",
    "        #         \"timedelta\": 24\n",
    "        #     }\n",
    "        # },\n",
    "        \"FTM\": {\n",
    "            \"1h\": {\n",
    "                \"file_path\": \"./data/test/binance/historical/scraped_binance_FTM_1h_2025-05-11_00:00:00.csv\",\n",
    "                \"timedelta\": 1\n",
    "            },\n",
    "            \"24h\": {\n",
    "                \"file_path\": \"./data/test/binance/historical/scraped_binance_FTM_1d_2025-05-11_00:00:00.csv\",\n",
    "                \"timedelta\": 24\n",
    "            }\n",
    "        },\n",
    "        # \"BAT\": {\n",
    "        #     \"1h\": {\n",
    "        #         \"file_path\": \"./data/test/binance/historical/scraped_binance_BAT_1h_2024-03-11_00:00:00.csv\",\n",
    "        #         \"timedelta\": 1\n",
    "        #     },\n",
    "        #     \"24h\": {\n",
    "        #         \"file_path\": \"./data/test/binance/historical/scraped_binance_BAT_1d_2024-03-11_00:00:00.csv\",\n",
    "        #         \"timedelta\": 24\n",
    "        #     }\n",
    "        # },\n",
    "    },\n",
    "\n",
    "    # \"bybit\": {\n",
    "    #     \"BTC\": {\n",
    "    #         \"1h\": {\n",
    "    #             \"file_path\": \"./data/test/bybit/historical/scraped_bybit_BTC_1h_2024-03-11_00:00:00.csv\",\n",
    "    #             \"timedelta\": 1\n",
    "    #         },\n",
    "    #         \"24h\": {\n",
    "    #             \"file_path\": \"./data/test/bybit/historical/scraped_bybit_BTC_1d_2024-03-11_00:00:00.csv\",\n",
    "    #             \"timedelta\": 24\n",
    "    #         }\n",
    "    #     },\n",
    "    #     \"ETH\": {\n",
    "    #         \"1h\": {\n",
    "    #             \"file_path\": \"./data/test/bybit/historical/scraped_bybit_ETH_1h_2024-03-11_00:00:00.csv\",\n",
    "    #             \"timedelta\": 1\n",
    "    #         },\n",
    "    #         \"24h\": {\n",
    "    #             \"file_path\": \"./data/test/bybit/historical/scraped_bybit_ETH_1d_2024-03-11_00:00:00.csv\",\n",
    "    #             \"timedelta\": 24\n",
    "    #         }\n",
    "    #     },\n",
    "    #     \"FTM\": {\n",
    "    #         \"1h\": {\n",
    "    #             \"file_path\": \"./data/test/bybit/historical/scraped_bybit_FTM_1h_2024-03-11_00:00:00.csv\",\n",
    "    #             \"timedelta\": 1\n",
    "    #         },\n",
    "    #         \"24h\": {\n",
    "    #             \"file_path\": \"./data/test/bybit/historical/scraped_bybit_FTM_1d_2024-03-11_00:00:00.csv\",\n",
    "    #             \"timedelta\": 24\n",
    "    #         }\n",
    "    #     },\n",
    "    # },\n",
    "\n",
    "    # \"glassnode\": {\n",
    "    #     \"BTC\": {\n",
    "    #         \"1h\": {\n",
    "    #             \"file_path\": \"./data/test/glassnode/historical/scraped_glassnode_BTC_1h_2024-03-11_00:00:00.csv\",\n",
    "    #             \"timedelta\": 1+1\n",
    "    #         },\n",
    "    #         \"24h\": {\n",
    "    #             \"file_path\": \"./data/test/glassnode/historical/scraped_glassnode_BTC_24h_2024-03-11_00:00:00.csv\",\n",
    "    #             \"timedelta\": 24+1\n",
    "    #         }\n",
    "    #     },\n",
    "    #     \"ETH\": {\n",
    "    #         \"1h\": {\n",
    "    #             \"file_path\": \"./data/test/glassnode/historical/scraped_glassnode_ETH_1h_2024-03-11_00:00:00.csv\",\n",
    "    #             \"timedelta\": 1+1\n",
    "    #         },\n",
    "    #         \"24h\": {\n",
    "    #             \"file_path\": \"./data/test/glassnode/historical/scraped_glassnode_ETH_24h_2024-03-11_00:00:00.csv\",\n",
    "    #             \"timedelta\": 24+1\n",
    "    #         }\n",
    "    #     },\n",
    "    # },\n",
    "\n",
    "    \"santiment\": {\n",
    "        \"BTC\": {\n",
    "            \"1h\": {\n",
    "                \"file_path\": \"./data/test/santiment/historical/scraped_santiment_BTC_1h_2025-05-11_00:00:00.csv\",\n",
    "                \"timedelta\": 1+1\n",
    "            },\n",
    "            \"24h\": {\n",
    "                \"file_path\": \"./data/test/santiment/historical/scraped_santiment_BTC_24h_2025-05-11_00:00:00.csv\",\n",
    "                \"timedelta\": 24+4\n",
    "            }\n",
    "        },\n",
    "        # \"ETH\": {\n",
    "        #     \"1h\": {\n",
    "        #         \"file_path\": \"./data/test/santiment/historical/scraped_santiment_ETH_1h_2024-03-11_00:00:00.csv\",\n",
    "        #         \"timedelta\": 1+1\n",
    "        #     },\n",
    "        #     \"24h\": {\n",
    "        #         \"file_path\": \"./data/test/santiment/historical/scraped_santiment_ETH_24h_2024-03-11_00:00:00.csv\",\n",
    "        #         \"timedelta\": 24+4\n",
    "        #     }\n",
    "        # },\n",
    "        \"FTM\": {\n",
    "            \"1h\": {\n",
    "                \"file_path\": \"./data/test/santiment/historical/scraped_santiment_FTM_1h_2025-05-11_00:00:00.csv\",\n",
    "                \"timedelta\": 1+1\n",
    "            },\n",
    "            \"24h\": {\n",
    "                \"file_path\": \"./data/test/santiment/historical/scraped_santiment_FTM_24h_2025-05-11_00:00:00.csv\",\n",
    "                \"timedelta\": 24+4\n",
    "            }\n",
    "        },\n",
    "\n",
    "    #    \"BAT\": {\n",
    "    #         \"1h\": {\n",
    "    #             \"file_path\": \"./data/test/santiment/historical/scraped_santiment_BAT_1h_2024-03-11_00:00:00.csv\",\n",
    "    #             \"timedelta\": 1+1\n",
    "    #         },\n",
    "    #         \"24h\": {\n",
    "    #             \"file_path\": \"./data/test/santiment/historical/scraped_santiment_BAT_24h_2024-03-11_00:00:00.csv\",\n",
    "    #             \"timedelta\": 24+4\n",
    "    #         }\n",
    "    #     },\n",
    "\n",
    "    #     \"USDT\": {\n",
    "    #         \"1h\": {\n",
    "    #             \"file_path\": \"./data/test/santiment/historical/scraped_santiment_USDT_1h_2024-03-11_00:00:00.csv\",\n",
    "    #             \"timedelta\": 1+1\n",
    "    #         },\n",
    "    #         \"24h\": {\n",
    "    #             \"file_path\": \"./data/test/santiment/historical/scraped_santiment_USDT_24h_2024-03-11_00:00:00.csv\",\n",
    "    #             \"timedelta\": 24+4\n",
    "    #         }\n",
    "    #     },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "modified_dfs = read_csvs_and_apply_timedelta(comb_dict)\n",
    "merged_df = merge_dataframes(modified_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Detect number of NaNs at the beginning of each metric (this metric hasn't been recorded/calculated)\n",
    "\n",
    "nan_counts = merged_df.isna().sum()\n",
    "nan_counts_df = pd.DataFrame(nan_counts)\n",
    "\n",
    "# Reset index to get column names into a column itself\n",
    "nan_counts_df.reset_index(inplace=True)\n",
    "nan_counts_df.columns = ['Metric', 'Value']\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(40, 24))\n",
    "ax.barh(nan_counts_df['Metric'], nan_counts_df['Value'], color='skyblue')\n",
    "\n",
    "# Setting labels and title\n",
    "ax.set_xlabel('Number of NaNs')\n",
    "ax.set_ylabel('Metrics')\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Detect rows without any NaN values\n",
    "rows_without_nan = merged_df.dropna().index\n",
    "\n",
    "# # Index of the first non-NaN row\n",
    "# first_non_nan_index = rows_without_nan[0] if not rows_without_nan.empty else None\n",
    "\n",
    "# Choose first index manually\n",
    "first_non_nan_index = merged_df.index[3000]\n",
    "print(\"datetime of the first index:\", first_non_nan_index)\n",
    "\n",
    "# Index of the last non-NaN row\n",
    "last_non_nan_index = rows_without_nan[-1] if not rows_without_nan.empty else None\n",
    "print(\"datetime of the last index:\", last_non_nan_index)\n",
    "\n",
    "# The actual slicing\n",
    "merged_df = merged_df.loc[first_non_nan_index:last_non_nan_index]\n",
    "\n",
    "merged_df = merged_df.fillna(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Add columns for time encoding\n",
    "\n",
    "# merged_df['week_sin'], merged_df['week_cos'] = zip(*merged_df.index.map(timestamp_to_week_cycle))\n",
    "merged_df['weekday'], merged_df['hour'] = zip(*merged_df.index.map(timestamp_to_weekday_hour))\n",
    "\n",
    "cols = merged_df.columns.tolist()  # Get the list of all columns\n",
    "reordered_cols = cols[-2:] + cols[:-2]  # Last two columns to the front\n",
    "merged_df = merged_df[reordered_cols]  # Apply new column order\n",
    "\n",
    "merged_df.to_csv(os.path.join(SAVE_FOLDER, \"merged_df.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Save final dataset as .npy files\n",
    "\n",
    "close_price_index = merged_df.columns.get_loc(price_column)\n",
    "price_array = np.array(merged_df.iloc[:, close_price_index])\n",
    "price_array = np.expand_dims(price_array.astype(np.float32), axis=1)\n",
    "\n",
    "tech_array = np.array(merged_df).astype(np.float32)\n",
    "\n",
    "print(\"price_array shape:\", price_array.shape)\n",
    "print(\"tech_array shape:\", tech_array.shape)\n",
    "\n",
    "np.save(os.path.join(SAVE_FOLDER, \"price_outfile.npy\"), price_array)\n",
    "np.save(os.path.join(SAVE_FOLDER, \"metrics_outfile.npy\"), tech_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[price_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Visualize training and validation intervals\n",
    "register_matplotlib_converters()\n",
    "\n",
    "# choose intervals for training and validation\n",
    "# training/validation split is 80/20\n",
    "data_frames = [\n",
    "    merged_df.iloc[0:960, [close_price_index]], merged_df.iloc[960:1200, [close_price_index]],\n",
    "    merged_df.iloc[1200:2160, [close_price_index]], merged_df.iloc[2160:2400, [close_price_index]],\n",
    "    merged_df.iloc[2400:3360, [close_price_index]], merged_df.iloc[3360:3600, [close_price_index]],\n",
    "    merged_df.iloc[3600:4560, [close_price_index]], merged_df.iloc[4560:4800, [close_price_index]],\n",
    "    merged_df.iloc[4800:5760, [close_price_index]], merged_df.iloc[5760:6000, [close_price_index]]\n",
    "]\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(nrows=5, ncols=2, figsize=(15, 10))\n",
    "axs = axs.flatten()  # Flatten to make it easier to iterate\n",
    "\n",
    "# Plot each DataFrame in its subplot\n",
    "for i, (ax, df) in enumerate(zip(axs, data_frames)):\n",
    "    # ax.plot(df.index, df['Value'])\n",
    "    ax.plot(df.index, df[price_column])\n",
    "\n",
    "    if i % 2 == 0:\n",
    "        ax.set_title('Training')\n",
    "    else:\n",
    "        ax.set_title('Validation')\n",
    "\n",
    "    # Optional: Format the x-axis with specific date formatting\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(5))  # Limit number of ticks\n",
    "\n",
    "# Automatically adjust plot parameters to give room and prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sane_ai_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
